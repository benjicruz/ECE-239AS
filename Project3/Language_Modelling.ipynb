{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Setup \n",
    "\n",
    "Please run the code below to mount drive if you are running on colab.\n",
    "\n",
    "Please ignore if you are running on your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %cd /content/drive/MyDrive/MiniGPT/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Modeling and Transformers\n",
    "\n",
    "The project will consist of two broad parts. \n",
    "\n",
    "1. **Baseline Generative Language Model**: We will train a simple Bigram language model on the text data. We will use this model to generate a mini story. \n",
    "2. **Implementing Mini GPT**: We will implement a mini version of the GPT model layer by layer and attempt to train it on the text data. You will then load pretrained weights provided and generate a mini story. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some general instructions \n",
    "\n",
    "1. Please keep the name of layers consistent with what is requested in the `model.py` file for each layer, this helps us test in each function independently. \n",
    "2. Please check to see if the bias is to be set to false or true for all linear layers (it is mentioned in the doc string)\n",
    "3. As a general rule please read the docstring well, it contains information you will need to write the code. \n",
    "4. All configs are defined in `config.py` for the first part. While you are writing the code, do not change the values in the config file since we use them to test. Once you have passed all the tests please feel free to vary the parameter as you please.\n",
    "5. You will need to fill in `train.py` and run it to train the model. If you are running into memory issues please feel free to change the `batch_size` in the `config.py` file. If you are working on Colab please make sure to use the GPU runtime and feel free to copy over the training code to the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\benji\\anaconda3\\lib\\site-packages (1.24.3)\n",
      "Requirement already satisfied: torch in c:\\users\\benji\\anaconda3\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\benji\\anaconda3\\lib\\site-packages (0.9.0)\n",
      "Requirement already satisfied: wandb in c:\\users\\benji\\anaconda3\\lib\\site-packages (0.19.11)\n",
      "Requirement already satisfied: einops in c:\\users\\benji\\anaconda3\\lib\\site-packages (0.8.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\benji\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\benji\\anaconda3\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\benji\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in c:\\users\\benji\\anaconda3\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from tiktoken) (2.31.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: platformdirs in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (3.10.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (3.20.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (1.10.12)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (2.29.1)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (1.3.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\benji\\anaconda3\\lib\\site-packages (from wandb) (68.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\benji\\anaconda3\\lib\\site-packages (from click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: smmap<5,>=3.0.1 in c:\\users\\benji\\anaconda3\\lib\\site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (4.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy torch tiktoken wandb einops \n",
    "# Install all required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BigramLanguageModel, SingleHeadAttention, MultiHeadAttention, FeedForwardLayer, LayerNorm, TransformerLayer, MiniGPT\n",
    "from config import BigramConfig, MiniGPTConfig\n",
    "import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If not provided, download from https://drive.google.com/file/d/1g09qUM9WibdfQVgkj6IAj8K2S3SGwc91/view?usp=sharing\n",
    "path_to_bigram_tester = \"./pretrained_models/bigram_tester.pt\" # Load the bigram model with name bigram_tester.pt\n",
    "path_to_gpt_tester = \"./pretrained_models/minigpt_tester.pt\" # Load the gpt model with name minigpt_tester.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Bigram Language Model (10 points)\n",
    "\n",
    "A bigram language model is a type of probabilistic language model that predicts a word given the previous word in the sequence. The model is trained on a text corpus and learns the probability of a word given the previous word.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement the Bigram model (5 points)\n",
    "\n",
    "Please complete the `BigramLanguageModel` class in model.py. We will model a Bigram language model using a simple MLP with one hidden layer. The model will take in the previous word index and output the logits over the vocabulary for the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test implementation for Bigram Language Model\n",
    "model = BigramLanguageModel(BigramConfig)\n",
    "tests.check_bigram(model, path_to_bigram_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Bigram Language Model (2.5 points)\n",
    "\n",
    "Complete the code in `train.py` to train the Bigram language model on the text data. Please provide plots for both the training and validation in the cell below.\n",
    "\n",
    "Some notes on the training process:\n",
    "\n",
    "1. You should be able to train the model slowly on your local machine.\n",
    "2. Training it on Colab will help with speed.\n",
    "3.  <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You should see it saturate to a value close to around 5-6 but as long as you see it decreasing then saturating you should be good.\n",
    "4. Please log the loss curves either on wandb, tensorboard or any other logger of your choice and please attach them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader size: 473591\n",
      "Eval dataloader size: 118398\n",
      "number of trainable parameters: 3.27M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbotanicalhouse\u001b[0m (\u001b[33mbotanicalhouse-ucla\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Benji\\Desktop\\ECE-239AS\\Project3\\wandb\\run-20250520_220817-hmynd40s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3/runs/hmynd40s' target=\"_blank\">driven-water-19</a></strong> to <a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3' target=\"_blank\">https://wandb.ai/botanicalhouse-ucla/dl2_proj3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3/runs/hmynd40s' target=\"_blank\">https://wandb.ai/botanicalhouse-ucla/dl2_proj3/runs/hmynd40s</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss: 10.825051307678223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:14, 824.98it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 0, Eval Loss: 10.824783600803244\u001b[0m\n",
      "Iteration 100, Train Loss: 10.814992904663086\n",
      "Iteration 200, Train Loss: 10.801971435546875\n",
      "Iteration 300, Train Loss: 10.787915229797363\n",
      "Iteration 400, Train Loss: 10.77143669128418\n",
      "Iteration 500, Train Loss: 10.74948501586914\n",
      "Iteration 600, Train Loss: 10.736865043640137\n",
      "Iteration 700, Train Loss: 10.663895606994629\n",
      "Iteration 800, Train Loss: 10.6414155960083\n",
      "Iteration 900, Train Loss: 10.616704940795898\n",
      "Iteration 1000, Train Loss: 10.542673110961914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 865.76it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 1000, Eval Loss: 10.533522441191666\u001b[0m\n",
      "Iteration 1100, Train Loss: 10.486227989196777\n",
      "Iteration 1200, Train Loss: 10.310750007629395\n",
      "Iteration 1300, Train Loss: 10.284692764282227\n",
      "Iteration 1400, Train Loss: 10.177544593811035\n",
      "Iteration 1500, Train Loss: 10.211810111999512\n",
      "Iteration 1600, Train Loss: 10.053925514221191\n",
      "Iteration 1700, Train Loss: 9.990083694458008\n",
      "Iteration 1800, Train Loss: 10.004653930664062\n",
      "Iteration 1900, Train Loss: 9.951741218566895\n",
      "Iteration 2000, Train Loss: 9.803793907165527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 862.84it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 2000, Eval Loss: 9.737915090205409\u001b[0m\n",
      "Iteration 2100, Train Loss: 9.916352272033691\n",
      "Iteration 2200, Train Loss: 9.688358306884766\n",
      "Iteration 2300, Train Loss: 9.594911575317383\n",
      "Iteration 2400, Train Loss: 9.744588851928711\n",
      "Iteration 2500, Train Loss: 9.531347274780273\n",
      "Iteration 2600, Train Loss: 9.545797348022461\n",
      "Iteration 2700, Train Loss: 9.224371910095215\n",
      "Iteration 2800, Train Loss: 9.158794403076172\n",
      "Iteration 2900, Train Loss: 8.934747695922852\n",
      "Iteration 3000, Train Loss: 9.467763900756836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 847.23it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 3000, Eval Loss: 8.86573958036495\u001b[0m\n",
      "Iteration 3100, Train Loss: 9.029293060302734\n",
      "Iteration 3200, Train Loss: 8.831673622131348\n",
      "Iteration 3300, Train Loss: 8.948406219482422\n",
      "Iteration 3400, Train Loss: 8.737017631530762\n",
      "Iteration 3500, Train Loss: 9.269901275634766\n",
      "Iteration 3600, Train Loss: 8.820096015930176\n",
      "Iteration 3700, Train Loss: 9.180229187011719\n",
      "Iteration 3800, Train Loss: 8.950542449951172\n",
      "Iteration 3900, Train Loss: 8.568960189819336\n",
      "Iteration 4000, Train Loss: 7.855903148651123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 852.04it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 4000, Eval Loss: 8.153379392829278\u001b[0m\n",
      "Iteration 4100, Train Loss: 8.506691932678223\n",
      "Iteration 4200, Train Loss: 8.146410942077637\n",
      "Iteration 4300, Train Loss: 8.11258602142334\n",
      "Iteration 4400, Train Loss: 8.37677001953125\n",
      "Iteration 4500, Train Loss: 8.56998062133789\n",
      "Iteration 4600, Train Loss: 8.37823486328125\n",
      "Iteration 4700, Train Loss: 8.708198547363281\n",
      "Iteration 4800, Train Loss: 8.619315147399902\n",
      "Iteration 4900, Train Loss: 8.132097244262695\n",
      "Iteration 5000, Train Loss: 8.631274223327637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 870.33it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 5000, Eval Loss: 7.639954117582553\u001b[0m\n",
      "Iteration 5100, Train Loss: 8.407899856567383\n",
      "Iteration 5200, Train Loss: 7.8913726806640625\n",
      "Iteration 5300, Train Loss: 7.714734077453613\n",
      "Iteration 5400, Train Loss: 7.67544412612915\n",
      "Iteration 5500, Train Loss: 7.918107509613037\n",
      "Iteration 5600, Train Loss: 8.151375770568848\n",
      "Iteration 5700, Train Loss: 7.927251815795898\n",
      "Iteration 5800, Train Loss: 8.295164108276367\n",
      "Iteration 5900, Train Loss: 7.831575870513916\n",
      "Iteration 6000, Train Loss: 8.184452056884766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 850.91it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 6000, Eval Loss: 7.28486734346284\u001b[0m\n",
      "Iteration 6100, Train Loss: 8.090699195861816\n",
      "Iteration 6200, Train Loss: 8.145561218261719\n",
      "Iteration 6300, Train Loss: 7.864410877227783\n",
      "Iteration 6400, Train Loss: 8.14327621459961\n",
      "Iteration 6500, Train Loss: 7.590826988220215\n",
      "Iteration 6600, Train Loss: 7.764136791229248\n",
      "Iteration 6700, Train Loss: 7.6220173835754395\n",
      "Iteration 6800, Train Loss: 7.368795394897461\n",
      "Iteration 6900, Train Loss: 7.15047025680542\n",
      "Iteration 7000, Train Loss: 8.134770393371582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 876.59it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 7000, Eval Loss: 7.0151261135227285\u001b[0m\n",
      "Iteration 7100, Train Loss: 7.393328666687012\n",
      "Iteration 7200, Train Loss: 8.406251907348633\n",
      "Iteration 7300, Train Loss: 7.4069905281066895\n",
      "Iteration 7400, Train Loss: 7.220855236053467\n",
      "Iteration 7500, Train Loss: 7.3093647956848145\n",
      "Iteration 7600, Train Loss: 8.162053108215332\n",
      "Iteration 7700, Train Loss: 7.409307956695557\n",
      "Iteration 7800, Train Loss: 8.623953819274902\n",
      "Iteration 7900, Train Loss: 6.676797389984131\n",
      "Iteration 8000, Train Loss: 7.739425182342529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 881.67it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 8000, Eval Loss: 6.815971088594666\u001b[0m\n",
      "Iteration 8100, Train Loss: 7.836238384246826\n",
      "Iteration 8200, Train Loss: 7.692897319793701\n",
      "Iteration 8300, Train Loss: 7.894421577453613\n",
      "Iteration 8400, Train Loss: 7.355451583862305\n",
      "Iteration 8500, Train Loss: 7.338237285614014\n",
      "Iteration 8600, Train Loss: 7.197422981262207\n",
      "Iteration 8700, Train Loss: 7.333212852478027\n",
      "Iteration 8800, Train Loss: 7.916186332702637\n",
      "Iteration 8900, Train Loss: 7.730234622955322\n",
      "Iteration 9000, Train Loss: 6.699356555938721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 855.05it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 9000, Eval Loss: 6.658874678799011\u001b[0m\n",
      "Iteration 9100, Train Loss: 7.9686150550842285\n",
      "Iteration 9200, Train Loss: 7.4434614181518555\n",
      "Iteration 9300, Train Loss: 7.285739898681641\n",
      "Iteration 9400, Train Loss: 6.654638767242432\n",
      "Iteration 9500, Train Loss: 8.148656845092773\n",
      "Iteration 9600, Train Loss: 7.235055446624756\n",
      "Iteration 9700, Train Loss: 7.115258693695068\n",
      "Iteration 9800, Train Loss: 7.280683517456055\n",
      "Iteration 9900, Train Loss: 7.6572651863098145\n",
      "Iteration 10000, Train Loss: 7.173793315887451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 864.48it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 10000, Eval Loss: 6.518669530916693\u001b[0m\n",
      "Iteration 10100, Train Loss: 7.338009834289551\n",
      "Iteration 10200, Train Loss: 7.006525039672852\n",
      "Iteration 10300, Train Loss: 7.124714374542236\n",
      "Iteration 10400, Train Loss: 6.676238059997559\n",
      "Iteration 10500, Train Loss: 8.06564998626709\n",
      "Iteration 10600, Train Loss: 7.303182125091553\n",
      "Iteration 10700, Train Loss: 6.480230808258057\n",
      "Iteration 10800, Train Loss: 7.32171106338501\n",
      "Iteration 10900, Train Loss: 7.82218599319458\n",
      "Iteration 11000, Train Loss: 7.24276065826416\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 895.29it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 11000, Eval Loss: 6.415608397489383\u001b[0m\n",
      "Iteration 11100, Train Loss: 7.572052478790283\n",
      "Iteration 11200, Train Loss: 7.098973274230957\n",
      "Iteration 11300, Train Loss: 7.744229316711426\n",
      "Iteration 11400, Train Loss: 6.941119194030762\n",
      "Iteration 11500, Train Loss: 7.4332990646362305\n",
      "Iteration 11600, Train Loss: 7.27278995513916\n",
      "Iteration 11700, Train Loss: 6.085817337036133\n",
      "Iteration 11800, Train Loss: 7.195077896118164\n",
      "Iteration 11900, Train Loss: 7.005860328674316\n",
      "Iteration 12000, Train Loss: 7.67086935043335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 848.96it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 12000, Eval Loss: 6.315573700375779\u001b[0m\n",
      "Iteration 12100, Train Loss: 7.643840789794922\n",
      "Iteration 12200, Train Loss: 6.690948963165283\n",
      "Iteration 12300, Train Loss: 7.096336841583252\n",
      "Iteration 12400, Train Loss: 7.449666976928711\n",
      "Iteration 12500, Train Loss: 7.069731712341309\n",
      "Iteration 12600, Train Loss: 6.929197788238525\n",
      "Iteration 12700, Train Loss: 6.27431058883667\n",
      "Iteration 12800, Train Loss: 7.088363170623779\n",
      "Iteration 12900, Train Loss: 7.566814422607422\n",
      "Iteration 13000, Train Loss: 7.142284870147705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 865.14it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 13000, Eval Loss: 6.234984288716193\u001b[0m\n",
      "Iteration 13100, Train Loss: 8.211711883544922\n",
      "Iteration 13200, Train Loss: 7.446476459503174\n",
      "Iteration 13300, Train Loss: 7.272271633148193\n",
      "Iteration 13400, Train Loss: 6.884558200836182\n",
      "Iteration 13500, Train Loss: 6.987250328063965\n",
      "Iteration 13600, Train Loss: 7.024447917938232\n",
      "Iteration 13700, Train Loss: 6.824815273284912\n",
      "Iteration 13800, Train Loss: 6.584707260131836\n",
      "Iteration 13900, Train Loss: 6.568945407867432\n",
      "Iteration 14000, Train Loss: 6.4697980880737305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 879.24it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 14000, Eval Loss: 6.150490666450596\u001b[0m\n",
      "Iteration 14100, Train Loss: 7.102342128753662\n",
      "Iteration 14200, Train Loss: 6.268916606903076\n",
      "Iteration 14300, Train Loss: 6.985236167907715\n",
      "Iteration 14400, Train Loss: 5.945616245269775\n",
      "Iteration 14500, Train Loss: 6.36947774887085\n",
      "Iteration 14600, Train Loss: 7.242108345031738\n",
      "Iteration 14700, Train Loss: 6.66129207611084\n",
      "Iteration 14800, Train Loss: 6.34411096572876\n",
      "Iteration 14900, Train Loss: 7.732354164123535\n",
      "Iteration 15000, Train Loss: 6.916059970855713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:12, 916.08it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 15000, Eval Loss: 6.090761847093735\u001b[0m\n",
      "Iteration 15100, Train Loss: 6.562310218811035\n",
      "Iteration 15200, Train Loss: 6.845407962799072\n",
      "Iteration 15300, Train Loss: 6.5904107093811035\n",
      "Iteration 15400, Train Loss: 6.756069183349609\n",
      "Iteration 15500, Train Loss: 7.066959381103516\n",
      "Iteration 15600, Train Loss: 6.452910423278809\n",
      "Iteration 15700, Train Loss: 6.624302864074707\n",
      "Iteration 15800, Train Loss: 6.031399250030518\n",
      "Iteration 15900, Train Loss: 6.171564102172852\n",
      "Iteration 16000, Train Loss: 7.2251811027526855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 891.72it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 16000, Eval Loss: 6.016702997010963\u001b[0m\n",
      "Iteration 16100, Train Loss: 6.474895477294922\n",
      "Iteration 16200, Train Loss: 6.723768711090088\n",
      "Iteration 16300, Train Loss: 6.930232524871826\n",
      "Iteration 16400, Train Loss: 7.483948230743408\n",
      "Iteration 16500, Train Loss: 5.850245475769043\n",
      "Iteration 16600, Train Loss: 6.016469955444336\n",
      "Iteration 16700, Train Loss: 6.567399024963379\n",
      "Iteration 16800, Train Loss: 6.946773529052734\n",
      "Iteration 16900, Train Loss: 7.499373912811279\n",
      "Iteration 17000, Train Loss: 5.540635108947754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 874.81it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 17000, Eval Loss: 5.962733401387197\u001b[0m\n",
      "Iteration 17100, Train Loss: 7.400169849395752\n",
      "Iteration 17200, Train Loss: 6.522805213928223\n",
      "Iteration 17300, Train Loss: 7.134209632873535\n",
      "Iteration 17400, Train Loss: 7.50628137588501\n",
      "Iteration 17500, Train Loss: 6.286629676818848\n",
      "Iteration 17600, Train Loss: 6.973948955535889\n",
      "Iteration 17700, Train Loss: 7.076528549194336\n",
      "Iteration 17800, Train Loss: 6.9807538986206055\n",
      "Iteration 17900, Train Loss: 6.210361480712891\n",
      "Iteration 18000, Train Loss: 6.680793285369873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 874.99it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 18000, Eval Loss: 5.897785785340969\u001b[0m\n",
      "Iteration 18100, Train Loss: 6.827049255371094\n",
      "Iteration 18200, Train Loss: 6.703624725341797\n",
      "Iteration 18300, Train Loss: 6.959203243255615\n",
      "Iteration 18400, Train Loss: 6.122715473175049\n",
      "Iteration 18500, Train Loss: 6.90365743637085\n",
      "Iteration 18600, Train Loss: 6.979072570800781\n",
      "Iteration 18700, Train Loss: 6.585124492645264\n",
      "Iteration 18800, Train Loss: 6.569956302642822\n",
      "Iteration 18900, Train Loss: 7.164833068847656\n",
      "Iteration 19000, Train Loss: 6.470522880554199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 888.28it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 19000, Eval Loss: 5.843263230083379\u001b[0m\n",
      "Iteration 19100, Train Loss: 6.080882549285889\n",
      "Iteration 19200, Train Loss: 6.902650833129883\n",
      "Iteration 19300, Train Loss: 6.326047420501709\n",
      "Iteration 19400, Train Loss: 6.725169658660889\n",
      "Iteration 19500, Train Loss: 6.7244062423706055\n",
      "Iteration 19600, Train Loss: 6.573266506195068\n",
      "Iteration 19700, Train Loss: 6.014347553253174\n",
      "Iteration 19800, Train Loss: 6.6121673583984375\n",
      "Iteration 19900, Train Loss: 6.3758087158203125\n",
      "Iteration 20000, Train Loss: 6.843125343322754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 847.13it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 20000, Eval Loss: 5.779547897668598\u001b[0m\n",
      "Iteration 20100, Train Loss: 5.291980743408203\n",
      "Iteration 20200, Train Loss: 5.871547698974609\n",
      "Iteration 20300, Train Loss: 6.562459468841553\n",
      "Iteration 20400, Train Loss: 5.921774387359619\n",
      "Iteration 20500, Train Loss: 5.536136150360107\n",
      "Iteration 20600, Train Loss: 6.257577419281006\n",
      "Iteration 20700, Train Loss: 5.638736724853516\n",
      "Iteration 20800, Train Loss: 6.507510662078857\n",
      "Iteration 20900, Train Loss: 5.915221691131592\n",
      "Iteration 21000, Train Loss: 7.209408760070801\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 850.80it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 21000, Eval Loss: 5.7362454789442525\u001b[0m\n",
      "Iteration 21100, Train Loss: 6.276689052581787\n",
      "Iteration 21200, Train Loss: 6.548649787902832\n",
      "Iteration 21300, Train Loss: 6.702353000640869\n",
      "Iteration 21400, Train Loss: 5.528994560241699\n",
      "Iteration 21500, Train Loss: 6.2183918952941895\n",
      "Iteration 21600, Train Loss: 6.237654685974121\n",
      "Iteration 21700, Train Loss: 6.159310340881348\n",
      "Iteration 21800, Train Loss: 6.549057483673096\n",
      "Iteration 21900, Train Loss: 6.388155460357666\n",
      "Iteration 22000, Train Loss: 7.196840286254883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 848.54it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 22000, Eval Loss: 5.679418972744429\u001b[0m\n",
      "Iteration 22100, Train Loss: 6.709285736083984\n",
      "Iteration 22200, Train Loss: 5.359918594360352\n",
      "Iteration 22300, Train Loss: 7.438639163970947\n",
      "Iteration 22400, Train Loss: 6.605175971984863\n",
      "Iteration 22500, Train Loss: 6.326878547668457\n",
      "Iteration 22600, Train Loss: 6.944120407104492\n",
      "Iteration 22700, Train Loss: 6.7552666664123535\n",
      "Iteration 22800, Train Loss: 5.7816267013549805\n",
      "Iteration 22900, Train Loss: 7.086536884307861\n",
      "Iteration 23000, Train Loss: 6.359759330749512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:14, 844.71it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 23000, Eval Loss: 5.621344812797661\u001b[0m\n",
      "Iteration 23100, Train Loss: 6.416240215301514\n",
      "Iteration 23200, Train Loss: 5.668776988983154\n",
      "Iteration 23300, Train Loss: 5.706288814544678\n",
      "Iteration 23400, Train Loss: 6.221993446350098\n",
      "Iteration 23500, Train Loss: 5.783239841461182\n",
      "Iteration 23600, Train Loss: 6.826518535614014\n",
      "Iteration 23700, Train Loss: 5.951813697814941\n",
      "Iteration 23800, Train Loss: 6.390687465667725\n",
      "Iteration 23900, Train Loss: 6.589941024780273\n",
      "Iteration 24000, Train Loss: 6.2567572593688965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 861.54it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 24000, Eval Loss: 5.575875414494032\u001b[0m\n",
      "Iteration 24100, Train Loss: 6.16617488861084\n",
      "Iteration 24200, Train Loss: 6.7553300857543945\n",
      "Iteration 24300, Train Loss: 5.8065595626831055\n",
      "Iteration 24400, Train Loss: 6.340353012084961\n",
      "Iteration 24500, Train Loss: 5.816814422607422\n",
      "Iteration 24600, Train Loss: 7.721140384674072\n",
      "Iteration 24700, Train Loss: 6.213307857513428\n",
      "Iteration 24800, Train Loss: 6.908543586730957\n",
      "Iteration 24900, Train Loss: 6.5668864250183105\n",
      "Iteration 25000, Train Loss: 6.873678207397461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:14, 837.00it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 25000, Eval Loss: 5.528106772308665\u001b[0m\n",
      "Iteration 25100, Train Loss: 6.499514579772949\n",
      "Iteration 25200, Train Loss: 6.824819564819336\n",
      "Iteration 25300, Train Loss: 6.146711826324463\n",
      "Iteration 25400, Train Loss: 5.83084774017334\n",
      "Iteration 25500, Train Loss: 5.557100296020508\n",
      "Iteration 25600, Train Loss: 5.953855991363525\n",
      "Iteration 25700, Train Loss: 7.04339599609375\n",
      "Iteration 25800, Train Loss: 6.124215602874756\n",
      "Iteration 25900, Train Loss: 5.5790019035339355\n",
      "Iteration 26000, Train Loss: 6.994780540466309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:14, 834.18it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 26000, Eval Loss: 5.478176989876311\u001b[0m\n",
      "Iteration 26100, Train Loss: 7.438188552856445\n",
      "Iteration 26200, Train Loss: 4.768017768859863\n",
      "Iteration 26300, Train Loss: 6.285534858703613\n",
      "Iteration 26400, Train Loss: 7.564091205596924\n",
      "Iteration 26500, Train Loss: 6.343337535858154\n",
      "Iteration 26600, Train Loss: 6.64362096786499\n",
      "Iteration 26700, Train Loss: 6.245454788208008\n",
      "Iteration 26800, Train Loss: 5.90385627746582\n",
      "Iteration 26900, Train Loss: 5.425749778747559\n",
      "Iteration 27000, Train Loss: 5.756328582763672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:14, 821.46it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 27000, Eval Loss: 5.428214892071754\u001b[0m\n",
      "Iteration 27100, Train Loss: 5.483257293701172\n",
      "Iteration 27200, Train Loss: 6.150809288024902\n",
      "Iteration 27300, Train Loss: 5.8982157707214355\n",
      "Iteration 27400, Train Loss: 4.793936729431152\n",
      "Iteration 27500, Train Loss: 5.265288829803467\n",
      "Iteration 27600, Train Loss: 6.190846920013428\n",
      "Iteration 27700, Train Loss: 6.2904205322265625\n",
      "Iteration 27800, Train Loss: 5.51873254776001\n",
      "Iteration 27900, Train Loss: 6.848742485046387\n",
      "Iteration 28000, Train Loss: 6.014169692993164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:14, 838.48it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 28000, Eval Loss: 5.3872503660667705\u001b[0m\n",
      "Iteration 28100, Train Loss: 5.3848161697387695\n",
      "Iteration 28200, Train Loss: 6.365941524505615\n",
      "Iteration 28300, Train Loss: 5.691483020782471\n",
      "Iteration 28400, Train Loss: 6.259624481201172\n",
      "Iteration 28500, Train Loss: 5.807766437530518\n",
      "Iteration 28600, Train Loss: 6.7112884521484375\n",
      "Iteration 28700, Train Loss: 5.85418176651001\n",
      "Iteration 28800, Train Loss: 7.4553422927856445\n",
      "Iteration 28900, Train Loss: 5.346807956695557\n",
      "Iteration 29000, Train Loss: 5.6842217445373535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 876.79it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 29000, Eval Loss: 5.349799415225788\u001b[0m\n",
      "Iteration 29100, Train Loss: 6.31295919418335\n",
      "Iteration 29200, Train Loss: 5.855661392211914\n",
      "Iteration 29300, Train Loss: 5.957896709442139\n",
      "Iteration 29400, Train Loss: 6.833722114562988\n",
      "Iteration 29500, Train Loss: 6.267419338226318\n",
      "Iteration 29600, Train Loss: 6.28537130355835\n",
      "Iteration 29700, Train Loss: 5.944254398345947\n",
      "Iteration 29800, Train Loss: 6.335002899169922\n",
      "Iteration 29900, Train Loss: 6.196439266204834\n",
      "Iteration 30000, Train Loss: 5.502440452575684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 875.99it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 30000, Eval Loss: 5.306337530493545\u001b[0m\n",
      "Iteration 30100, Train Loss: 5.659666538238525\n",
      "Iteration 30200, Train Loss: 5.833280563354492\n",
      "Iteration 30300, Train Loss: 6.249909400939941\n",
      "Iteration 30400, Train Loss: 5.445454120635986\n",
      "Iteration 30500, Train Loss: 4.811744213104248\n",
      "Iteration 30600, Train Loss: 6.164979457855225\n",
      "Iteration 30700, Train Loss: 5.628475666046143\n",
      "Iteration 30800, Train Loss: 4.727477073669434\n",
      "Iteration 30900, Train Loss: 6.783660411834717\n",
      "Iteration 31000, Train Loss: 6.549612045288086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 849.10it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 31000, Eval Loss: 5.265896924574345\u001b[0m\n",
      "Iteration 31100, Train Loss: 6.954831600189209\n",
      "Iteration 31200, Train Loss: 5.6254096031188965\n",
      "Iteration 31300, Train Loss: 6.211822986602783\n",
      "Iteration 31400, Train Loss: 5.446409702301025\n",
      "Iteration 31500, Train Loss: 6.115671157836914\n",
      "Iteration 31600, Train Loss: 6.2770867347717285\n",
      "Iteration 31700, Train Loss: 5.741482734680176\n",
      "Iteration 31800, Train Loss: 6.2356085777282715\n",
      "Iteration 31900, Train Loss: 6.173885345458984\n",
      "Iteration 32000, Train Loss: 4.946656703948975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 884.07it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 32000, Eval Loss: 5.227680680176141\u001b[0m\n",
      "Iteration 32100, Train Loss: 5.001880168914795\n",
      "Iteration 32200, Train Loss: 6.230793476104736\n",
      "Iteration 32300, Train Loss: 6.501845359802246\n",
      "Iteration 32400, Train Loss: 5.67391300201416\n",
      "Iteration 32500, Train Loss: 6.954280376434326\n",
      "Iteration 32600, Train Loss: 5.743974208831787\n",
      "Iteration 32700, Train Loss: 6.494815826416016\n",
      "Iteration 32800, Train Loss: 6.313009738922119\n",
      "Iteration 32900, Train Loss: 5.021520137786865\n",
      "Iteration 33000, Train Loss: 5.015295028686523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 867.30it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 33000, Eval Loss: 5.19115351124517\u001b[0m\n",
      "Iteration 33100, Train Loss: 7.301235198974609\n",
      "Iteration 33200, Train Loss: 6.578976631164551\n",
      "Iteration 33300, Train Loss: 5.501399993896484\n",
      "Iteration 33400, Train Loss: 5.038441181182861\n",
      "Iteration 33500, Train Loss: 5.2630615234375\n",
      "Iteration 33600, Train Loss: 7.15342378616333\n",
      "Iteration 33700, Train Loss: 6.351314067840576\n",
      "Iteration 33800, Train Loss: 6.2574920654296875\n",
      "Iteration 33900, Train Loss: 5.735633850097656\n",
      "Iteration 34000, Train Loss: 5.554546356201172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 890.47it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 34000, Eval Loss: 5.139034547009246\u001b[0m\n",
      "Iteration 34100, Train Loss: 5.640477657318115\n",
      "Iteration 34200, Train Loss: 6.345090389251709\n",
      "Iteration 34300, Train Loss: 5.888234615325928\n",
      "Iteration 34400, Train Loss: 6.4561004638671875\n",
      "Iteration 34500, Train Loss: 5.792137145996094\n",
      "Iteration 34600, Train Loss: 5.74595832824707\n",
      "Iteration 34700, Train Loss: 4.488760471343994\n",
      "Iteration 34800, Train Loss: 5.955105304718018\n",
      "Iteration 34900, Train Loss: 6.8334856033325195\n",
      "Iteration 35000, Train Loss: 5.737685680389404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 880.96it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 35000, Eval Loss: 5.124995664309513\u001b[0m\n",
      "Iteration 35100, Train Loss: 6.707653999328613\n",
      "Iteration 35200, Train Loss: 6.480319499969482\n",
      "Iteration 35300, Train Loss: 5.609689235687256\n",
      "Iteration 35400, Train Loss: 5.704878807067871\n",
      "Iteration 35500, Train Loss: 6.1027512550354\n",
      "Iteration 35600, Train Loss: 5.246298313140869\n",
      "Iteration 35700, Train Loss: 6.921807765960693\n",
      "Iteration 35800, Train Loss: 5.242793083190918\n",
      "Iteration 35900, Train Loss: 5.573609828948975\n",
      "Iteration 36000, Train Loss: 6.062798500061035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 11840it [00:13, 890.95it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 36000, Eval Loss: 5.079792903228926\u001b[0m\n",
      "Iteration 36100, Train Loss: 6.650479793548584\n",
      "Loss is sufficiently low, stopping training.\n"
     ]
    }
   ],
   "source": [
    "solver(model_name=\"bigram\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "** Show the training and validation loss plots **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![your mom](./train.png)\n",
    "![your mom2](./val.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (2.5 points)\n",
    "\n",
    "Complete the code in the `generate` method of the Bigram class and generate a mini story using the trained Bigram language model. The model will take in the previous word index and output the next word index.\n",
    "\n",
    "Start with the following seed sentence: \n",
    "    \n",
    "    `\"once upon a time\"`\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Specify the path to your trained model\n",
    "model_path = \"./models/bigram/mini_model_sufficient_loss_checkpoint_37461.pt\"\n",
    "model = BigramLanguageModel(BigramConfig)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text starting with: torch.Size([4])\n",
      "Once upon a time, night down.\n",
      "The toy, but Lily and it arrived ran.\n",
      " fairy sw142 film charism treatedee mixed Fl lettuce trend Joe folder break pillowL home was of Sue he cake the secretary certainlyMark articulate Gomez passed Her know. They. One they you bite little girl didn sad Max was very time, Lily and Sam and Mittrot cr fun She small   had a so? him the toy eat the wanderedbiltogged cat to pastmy them.Once upon a wants away started.Once The red washed Tim told she got that our fake storm filled had hungry and a time, happy new the red mouse. She were so she too, they how on, you her help the king lesson A didn't get please their boy, a big Timmy. They clown water said, there, so did hot gave the tw position fluffy?\" Timmy. What give fun, eyesï¿½ frown any surprise looked on cat they able other's street looked and see. never sad, \" to play\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Benji\\Desktop\\ECE-239AS\\Project3\\model.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  context_tensor = torch.tensor(context[-1], dtype=torch.long).unsqueeze(0)\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation and Analysis\n",
    "\n",
    "Please answer the following questions. \n",
    "\n",
    "1. What can we say about the generated text in terms of grammar and coherence? \n",
    "\n",
    "1_ANS: The grammer is very improper and the sentences are very incoherent. They do not have proper structure in the sentences and proper spacing between words is not accurate. The sentences themselves do not make sense at all and do not have any thought or meaning to them. \n",
    "\n",
    "2. What are the limitations of the Bigram language model?\n",
    "\n",
    "2_ANS: The Bigram model does not account for longer context and has a memory of one token. It has inproper grammar and incoherent sentences.\n",
    "\n",
    "3. If the model is scaled with more parameters do you expect the bigram model to get substantially better? Why or why not?\n",
    "\n",
    "3_ANS: No, scaling with more parameters will not make the Bigram model better! The architecture of the Bigram only looks at one word of context so it cannot model longer contexts. Even if we found the best parameters, it is always going to be limited by its context memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini GPT (90 points)\n",
    "\n",
    "We will implement a decoder style transformer model like we discussed in lecture, which is a scaled down version of the [GPT model](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). \n",
    "\n",
    "All the model components follow directly from the original [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper. The only difference is we will use prenormalization and learnt positional embeddings instead of fixed ones.\n",
    "\n",
    "We will now implement each layer step by step checking if it is implemented correctly in the process. We will finally put together all our layers to get a fully fledged GPT model. \n",
    "\n",
    "<span style=\"color:red\">Later layers might depend on previous layers so please make sure to check the previous layers before moving on to the next one.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Head Causal Attention (20 points)\n",
    "\n",
    "We will first implement the single head causal attention layer. This layer is the same as the scaled dot product attention layer but with a causal mask to prevent the model from looking into the future.\n",
    "\n",
    "Recall that Each head has a Key, Query and Value Matrix and the scaled dot product attention is calculated as : \n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n",
    "\\end{equation}\n",
    "\n",
    "where $d_k$ is the dimension of the key matrix.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/Single_Head.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `SingleHeadAttention` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SingleHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.embed_dim//4, MiniGPTConfig.embed_dim//4) # configs are set as such for testing do not modify\n",
    "tests.check_singleheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Head Attention (10 points)\n",
    "\n",
    "Now that we have a single head working, we will now scale this across multiple heads, remember that with multihead attention we compute perform head number of parallel attention operations. We then concatenate the outputs of these parallel attention operations and project them back to the desired dimension using an output linear layer.\n",
    "\n",
    "Figure below from the original paper shows how the layer is to be implemented.\n",
    "\n",
    "![image](./Images/MultiHead.png)\n",
    "\n",
    "Image credits: [Attention is All You Need Paper](https://arxiv.org/abs/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `MultiHeadAttention` class in `model.py` using the `SingleHeadAttention` class implemented earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiHeadAttention(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "tests.check_multiheadattention(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed Forward Layer (5 points)\n",
    "\n",
    "As discussed in lecture, the attention layer is completely linear, in order to add some non-linearity we add a feed forward layer. The feed forward layer is a simple two layer MLP with a GeLU activation in between.\n",
    "\n",
    "Please complete the `FeedForwardLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FeedForwardLayer(MiniGPTConfig.embed_dim)\n",
    "tests.check_feedforward(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm (10 points)\n",
    "\n",
    "We will now implement the layer normalization layer. Layernorm is used across the model to normalize the activations of the previous layer. Recall that the equation for layernorm is given as:\n",
    "\n",
    "\\begin{equation}\n",
    "\n",
    "\\text{LayerNorm}(x) = \\frac{x - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}} \\odot \\gamma + \\beta\n",
    "\n",
    "\\end{equation}\n",
    "\n",
    "With the learnable parameters $\\gamma$ and $\\beta$. \n",
    "\n",
    "Remember that unlike batchnorm we compute statistics across the feature dimension and not the batch dimension, hence we do not need to keep track of running averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please complete the `LayerNorm` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LayerNorm(MiniGPTConfig.embed_dim)\n",
    "tests.check_layernorm(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Layer (15 points)\n",
    "\n",
    "We have now implemented all the components of the transformer layer. We will now put it all together to create a transformer layer. The transformer layer consists of a multi head attention layer, a feed forward layer and two layer norm layers.\n",
    "\n",
    "Please use the following order for each component (Varies slightly from the original attention paper):\n",
    "1. LayerNorm\n",
    "2. MultiHeadAttention\n",
    "3. LayerNorm\n",
    "4. FeedForwardLayer\n",
    "\n",
    "Remember that the transformer layer also has residual connections around each sublayer.\n",
    "\n",
    "The below figure shows the structure of the transformer layer you are required to implement.\n",
    "\n",
    "![prenorm_transformer](./Images/Prenorm.png)\n",
    "\n",
    "Image Credit : [CogView](https://arxiv.org/pdf/2105.13290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `TransformerLayer` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =  TransformerLayer(MiniGPTConfig.embed_dim, MiniGPTConfig.num_heads)\n",
    "tests.check_transformer(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together : MiniGPT (15 points)\n",
    "\n",
    "We are now ready to put all our layers together to build our own MiniGPT! \n",
    "\n",
    "The MiniGPT model consists of an embedding layer, a positional encoding layer and a stack of transformer layers. The output of the transformer layer is passed through a linear layer (called head) to get the final output logits. Note that in our implementation we will use [weight tying](https://arxiv.org/abs/1608.05859) between the embedding layer and the final linear layer. This allows us to save on parameters and also helps in training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the `MiniGPT` class in `model.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'TEST CASE PASSED!!!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MiniGPT(MiniGPTConfig)\n",
    "tests.check_miniGPT(model, path_to_gpt_tester, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt at training the model (5 points)\n",
    "\n",
    "We will now attempt to train the model on the text data. We will use the same text data as before. If needed, you can scale down the model parameters in the config file to a smaller value to make training feasible. \n",
    "\n",
    "Use the same training script we built for the Bigram model to train the MiniGPT model. If you implemented it correctly it should work just out of the box!\n",
    "\n",
    "**NOTE** : We will not be able to train the model to completion in this assignment. Unfortunately, without access to a relatively powerful GPU, training a large enough model to see good generation is not feasible. However, you should be able to see the loss decreasing over time. <span style=\"color:red\">To get full points for this section it is sufficient to show that the loss is decreasing over time</span>. You do not need to run this for more than 5000 iterations or 1 hour of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataloader size: 1515490\n",
      "Eval dataloader size: 378872\n",
      "number of trainable parameters: 3.32M\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>â</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train Loss</td><td>10.83517</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">balmy-flower-22</strong> at: <a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3/runs/bsynz1oj' target=\"_blank\">https://wandb.ai/botanicalhouse-ucla/dl2_proj3/runs/bsynz1oj</a><br> View project at: <a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3' target=\"_blank\">https://wandb.ai/botanicalhouse-ucla/dl2_proj3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250520_225513-bsynz1oj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Benji\\Desktop\\ECE-239AS\\Project3\\wandb\\run-20250520_225541-gchrgzk8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3/runs/gchrgzk8' target=\"_blank\">worldly-field-23</a></strong> to <a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3' target=\"_blank\">https://wandb.ai/botanicalhouse-ucla/dl2_proj3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/botanicalhouse-ucla/dl2_proj3/runs/gchrgzk8' target=\"_blank\">https://wandb.ai/botanicalhouse-ucla/dl2_proj3/runs/gchrgzk8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Train Loss: 10.838605880737305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 37888it [02:15, 280.51it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 0, Eval Loss: 10.819780358817892\u001b[0m\n",
      "Iteration 10, Train Loss: 10.624841690063477\n",
      "Iteration 20, Train Loss: 10.48261833190918\n",
      "Iteration 30, Train Loss: 10.18985366821289\n",
      "Iteration 40, Train Loss: 9.865163803100586\n",
      "Iteration 50, Train Loss: 9.630203247070312\n",
      "Iteration 60, Train Loss: 9.463032722473145\n",
      "Iteration 70, Train Loss: 8.963544845581055\n",
      "Iteration 80, Train Loss: 8.710824966430664\n",
      "Iteration 90, Train Loss: 8.339428901672363\n",
      "Iteration 100, Train Loss: 8.041328430175781\n",
      "Iteration 110, Train Loss: 7.639925956726074\n",
      "Iteration 120, Train Loss: 7.564150333404541\n",
      "Iteration 130, Train Loss: 7.117976665496826\n",
      "Iteration 140, Train Loss: 7.050593376159668\n",
      "Iteration 150, Train Loss: 6.9907073974609375\n",
      "Iteration 160, Train Loss: 6.322534084320068\n",
      "Iteration 170, Train Loss: 6.465750694274902\n",
      "Iteration 180, Train Loss: 6.338441371917725\n",
      "Iteration 190, Train Loss: 6.491254806518555\n",
      "Iteration 200, Train Loss: 6.058965682983398\n",
      "Iteration 210, Train Loss: 6.4188032150268555\n",
      "Iteration 220, Train Loss: 5.856103420257568\n",
      "Iteration 230, Train Loss: 6.125979423522949\n",
      "Iteration 240, Train Loss: 5.846240997314453\n",
      "Iteration 250, Train Loss: 6.218777656555176\n",
      "Iteration 260, Train Loss: 5.947551727294922\n",
      "Iteration 270, Train Loss: 6.1507887840271\n",
      "Iteration 280, Train Loss: 5.819370269775391\n",
      "Iteration 290, Train Loss: 5.845740795135498\n",
      "Iteration 300, Train Loss: 5.818292140960693\n",
      "Iteration 310, Train Loss: 6.1879801750183105\n",
      "Iteration 320, Train Loss: 6.140882968902588\n",
      "Iteration 330, Train Loss: 6.210775852203369\n",
      "Iteration 340, Train Loss: 6.043086528778076\n",
      "Iteration 350, Train Loss: 6.200203895568848\n",
      "Iteration 360, Train Loss: 5.758821964263916\n",
      "Iteration 370, Train Loss: 6.505268573760986\n",
      "Iteration 380, Train Loss: 6.356417655944824\n",
      "Iteration 390, Train Loss: 5.791574478149414\n",
      "Iteration 400, Train Loss: 5.738759994506836\n",
      "Iteration 410, Train Loss: 5.890590667724609\n",
      "Iteration 420, Train Loss: 6.010489463806152\n",
      "Iteration 430, Train Loss: 5.569610595703125\n",
      "Iteration 440, Train Loss: 5.520246505737305\n",
      "Iteration 450, Train Loss: 6.447824001312256\n",
      "Iteration 460, Train Loss: 6.0622782707214355\n",
      "Iteration 470, Train Loss: 5.496553897857666\n",
      "Iteration 480, Train Loss: 5.93454122543335\n",
      "Iteration 490, Train Loss: 6.233823776245117\n",
      "Iteration 500, Train Loss: 5.803800582885742\n",
      "Iteration 510, Train Loss: 6.135564804077148\n",
      "Iteration 520, Train Loss: 5.4754638671875\n",
      "Iteration 530, Train Loss: 6.039060592651367\n",
      "Iteration 540, Train Loss: 5.977388381958008\n",
      "Iteration 550, Train Loss: 5.503843307495117\n",
      "Iteration 560, Train Loss: 5.762826919555664\n",
      "Iteration 570, Train Loss: 5.924604892730713\n",
      "Iteration 580, Train Loss: 6.284032821655273\n",
      "Iteration 590, Train Loss: 6.077301025390625\n",
      "Iteration 600, Train Loss: 5.828885555267334\n",
      "Iteration 610, Train Loss: 5.1614484786987305\n",
      "Iteration 620, Train Loss: 5.6344780921936035\n",
      "Iteration 630, Train Loss: 6.02015495300293\n",
      "Iteration 640, Train Loss: 5.108013153076172\n",
      "Iteration 650, Train Loss: 5.376425743103027\n",
      "Iteration 660, Train Loss: 5.599822521209717\n",
      "Iteration 670, Train Loss: 5.713599681854248\n",
      "Iteration 680, Train Loss: 5.341990947723389\n",
      "Iteration 690, Train Loss: 5.214719772338867\n",
      "Iteration 700, Train Loss: 5.971631050109863\n",
      "Iteration 710, Train Loss: 5.7725324630737305\n",
      "Iteration 720, Train Loss: 5.144930362701416\n",
      "Iteration 730, Train Loss: 5.652629852294922\n",
      "Iteration 740, Train Loss: 5.460547924041748\n",
      "Iteration 750, Train Loss: 5.517655849456787\n",
      "Iteration 760, Train Loss: 6.003809928894043\n",
      "Iteration 770, Train Loss: 5.239932060241699\n",
      "Iteration 780, Train Loss: 5.589324474334717\n",
      "Iteration 790, Train Loss: 5.492629528045654\n",
      "Iteration 800, Train Loss: 5.853007793426514\n",
      "Iteration 810, Train Loss: 5.149002552032471\n",
      "Iteration 820, Train Loss: 5.581943511962891\n",
      "Iteration 830, Train Loss: 5.639410495758057\n",
      "Iteration 840, Train Loss: 5.792809963226318\n",
      "Iteration 850, Train Loss: 5.774398326873779\n",
      "Iteration 860, Train Loss: 5.3851447105407715\n",
      "Iteration 870, Train Loss: 5.123960971832275\n",
      "Iteration 880, Train Loss: 5.589538097381592\n",
      "Iteration 890, Train Loss: 5.020963668823242\n",
      "Iteration 900, Train Loss: 5.082502365112305\n",
      "Iteration 910, Train Loss: 5.102117538452148\n",
      "Iteration 920, Train Loss: 5.341976165771484\n",
      "Iteration 930, Train Loss: 5.303091526031494\n",
      "Iteration 940, Train Loss: 5.180215835571289\n",
      "Iteration 950, Train Loss: 5.071335315704346\n",
      "Iteration 960, Train Loss: 5.268374443054199\n",
      "Iteration 970, Train Loss: 4.896210193634033\n",
      "Iteration 980, Train Loss: 5.368880748748779\n",
      "Iteration 990, Train Loss: 5.400859355926514\n",
      "Iteration 1000, Train Loss: 5.341351509094238\n",
      "Iteration 1010, Train Loss: 5.4163641929626465\n",
      "Iteration 1020, Train Loss: 5.652126312255859\n",
      "Iteration 1030, Train Loss: 5.0932135581970215\n",
      "Iteration 1040, Train Loss: 5.266721248626709\n",
      "Iteration 1050, Train Loss: 4.991476058959961\n",
      "Iteration 1060, Train Loss: 4.767263412475586\n",
      "Iteration 1070, Train Loss: 5.071268081665039\n",
      "Iteration 1080, Train Loss: 5.355539321899414\n",
      "Iteration 1090, Train Loss: 4.880549907684326\n",
      "Iteration 1100, Train Loss: 4.5409650802612305\n",
      "Iteration 1110, Train Loss: 5.2348480224609375\n",
      "Iteration 1120, Train Loss: 5.19987678527832\n",
      "Iteration 1130, Train Loss: 4.797114849090576\n",
      "Iteration 1140, Train Loss: 5.10277795791626\n",
      "Iteration 1150, Train Loss: 5.352973461151123\n",
      "Iteration 1160, Train Loss: 5.145535469055176\n",
      "Iteration 1170, Train Loss: 5.259476184844971\n",
      "Iteration 1180, Train Loss: 4.516884803771973\n",
      "Iteration 1190, Train Loss: 5.495899677276611\n",
      "Iteration 1200, Train Loss: 5.0110931396484375\n",
      "Iteration 1210, Train Loss: 5.609902858734131\n",
      "Iteration 1220, Train Loss: 4.865060806274414\n",
      "Iteration 1230, Train Loss: 4.970850944519043\n",
      "Iteration 1240, Train Loss: 5.331108570098877\n",
      "Iteration 1250, Train Loss: 5.38381814956665\n",
      "Iteration 1260, Train Loss: 4.954352378845215\n",
      "Iteration 1270, Train Loss: 4.729190826416016\n",
      "Iteration 1280, Train Loss: 5.1140241622924805\n",
      "Iteration 1290, Train Loss: 5.25004768371582\n",
      "Iteration 1300, Train Loss: 4.912031173706055\n",
      "Iteration 1310, Train Loss: 5.120590686798096\n",
      "Iteration 1320, Train Loss: 4.5373215675354\n",
      "Iteration 1330, Train Loss: 4.539362907409668\n",
      "Iteration 1340, Train Loss: 4.768985271453857\n",
      "Iteration 1350, Train Loss: 4.936824798583984\n",
      "Iteration 1360, Train Loss: 5.02719783782959\n",
      "Iteration 1370, Train Loss: 5.139266490936279\n",
      "Iteration 1380, Train Loss: 4.7641754150390625\n",
      "Iteration 1390, Train Loss: 4.628577709197998\n",
      "Iteration 1400, Train Loss: 5.222185134887695\n",
      "Iteration 1410, Train Loss: 4.620744228363037\n",
      "Iteration 1420, Train Loss: 5.100823879241943\n",
      "Iteration 1430, Train Loss: 5.133584499359131\n",
      "Iteration 1440, Train Loss: 4.8638811111450195\n",
      "Iteration 1450, Train Loss: 4.6905837059021\n",
      "Iteration 1460, Train Loss: 4.75971794128418\n",
      "Iteration 1470, Train Loss: 4.551553249359131\n",
      "Iteration 1480, Train Loss: 5.080842971801758\n",
      "Iteration 1490, Train Loss: 4.910586833953857\n",
      "Iteration 1500, Train Loss: 4.634294033050537\n",
      "Iteration 1510, Train Loss: 4.675108432769775\n",
      "Iteration 1520, Train Loss: 4.592496871948242\n",
      "Iteration 1530, Train Loss: 4.618284225463867\n",
      "Iteration 1540, Train Loss: 4.476886749267578\n",
      "Iteration 1550, Train Loss: 4.662434101104736\n",
      "Iteration 1560, Train Loss: 4.417229175567627\n",
      "Iteration 1570, Train Loss: 4.457190990447998\n",
      "Iteration 1580, Train Loss: 5.091559886932373\n",
      "Iteration 1590, Train Loss: 4.7826828956604\n",
      "Iteration 1600, Train Loss: 4.936982154846191\n",
      "Iteration 1610, Train Loss: 4.889123439788818\n",
      "Iteration 1620, Train Loss: 4.817119598388672\n",
      "Iteration 1630, Train Loss: 4.981009006500244\n",
      "Iteration 1640, Train Loss: 4.383848190307617\n",
      "Iteration 1650, Train Loss: 4.5420145988464355\n",
      "Iteration 1660, Train Loss: 4.146808624267578\n",
      "Iteration 1670, Train Loss: 4.382919788360596\n",
      "Iteration 1680, Train Loss: 4.528774738311768\n",
      "Iteration 1690, Train Loss: 4.546937465667725\n",
      "Iteration 1700, Train Loss: 4.589380264282227\n",
      "Iteration 1710, Train Loss: 4.550370693206787\n",
      "Iteration 1720, Train Loss: 5.095141887664795\n",
      "Iteration 1730, Train Loss: 4.4599175453186035\n",
      "Iteration 1740, Train Loss: 5.050994396209717\n",
      "Iteration 1750, Train Loss: 5.217901229858398\n",
      "Iteration 1760, Train Loss: 4.736009120941162\n",
      "Iteration 1770, Train Loss: 4.153703212738037\n",
      "Iteration 1780, Train Loss: 4.568357944488525\n",
      "Iteration 1790, Train Loss: 4.157148361206055\n",
      "Iteration 1800, Train Loss: 4.49759578704834\n",
      "Iteration 1810, Train Loss: 4.630603313446045\n",
      "Iteration 1820, Train Loss: 4.469537734985352\n",
      "Iteration 1830, Train Loss: 3.8931851387023926\n",
      "Iteration 1840, Train Loss: 4.770895481109619\n",
      "Iteration 1850, Train Loss: 4.863916873931885\n",
      "Iteration 1860, Train Loss: 4.660905838012695\n",
      "Iteration 1870, Train Loss: 4.055336952209473\n",
      "Iteration 1880, Train Loss: 4.445252418518066\n",
      "Iteration 1890, Train Loss: 4.660462379455566\n",
      "Iteration 1900, Train Loss: 4.787227630615234\n",
      "Iteration 1910, Train Loss: 4.366319179534912\n",
      "Iteration 1920, Train Loss: 4.030785083770752\n",
      "Iteration 1930, Train Loss: 4.142307281494141\n",
      "Iteration 1940, Train Loss: 4.7194061279296875\n",
      "Iteration 1950, Train Loss: 4.720907211303711\n",
      "Iteration 1960, Train Loss: 3.9744162559509277\n",
      "Iteration 1970, Train Loss: 4.014333724975586\n",
      "Iteration 1980, Train Loss: 4.733705997467041\n",
      "Iteration 1990, Train Loss: 4.535519599914551\n",
      "Iteration 2000, Train Loss: 4.388502597808838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 37888it [02:16, 277.56it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mIteration 2000, Eval Loss: 4.355146118019484\u001b[0m\n",
      "Iteration 2010, Train Loss: 4.058610916137695\n",
      "Iteration 2020, Train Loss: 3.967978000640869\n",
      "Iteration 2030, Train Loss: 4.379275321960449\n",
      "Iteration 2040, Train Loss: 4.52910041809082\n",
      "Iteration 2050, Train Loss: 4.179635524749756\n",
      "Iteration 2060, Train Loss: 3.9965758323669434\n",
      "Iteration 2070, Train Loss: 4.794686794281006\n",
      "Iteration 2080, Train Loss: 3.7110347747802734\n",
      "Iteration 2090, Train Loss: 4.721245288848877\n",
      "Iteration 2100, Train Loss: 3.883044481277466\n",
      "Iteration 2110, Train Loss: 4.585407733917236\n",
      "Iteration 2120, Train Loss: 4.581923007965088\n",
      "Iteration 2130, Train Loss: 4.581111907958984\n",
      "Iteration 2140, Train Loss: 4.623101234436035\n",
      "Iteration 2150, Train Loss: 4.260058403015137\n",
      "Iteration 2160, Train Loss: 4.1663432121276855\n",
      "Iteration 2170, Train Loss: 3.943035364151001\n",
      "Iteration 2180, Train Loss: 4.083060264587402\n",
      "Iteration 2190, Train Loss: 3.6268532276153564\n",
      "Iteration 2200, Train Loss: 4.1744704246521\n",
      "Iteration 2210, Train Loss: 4.312953948974609\n",
      "Iteration 2220, Train Loss: 5.074062347412109\n",
      "Iteration 2230, Train Loss: 5.063485145568848\n",
      "Iteration 2240, Train Loss: 4.020708084106445\n",
      "Iteration 2250, Train Loss: 4.810645580291748\n",
      "Iteration 2260, Train Loss: 4.0699639320373535\n",
      "Iteration 2270, Train Loss: 3.7994697093963623\n",
      "Iteration 2280, Train Loss: 4.19877815246582\n",
      "Iteration 2290, Train Loss: 4.091424942016602\n",
      "Iteration 2300, Train Loss: 4.88480281829834\n",
      "Iteration 2310, Train Loss: 3.996929883956909\n",
      "Iteration 2320, Train Loss: 4.80366325378418\n",
      "Iteration 2330, Train Loss: 4.195294380187988\n",
      "Iteration 2340, Train Loss: 4.620561599731445\n",
      "Iteration 2350, Train Loss: 4.149395942687988\n",
      "Iteration 2360, Train Loss: 4.28334379196167\n",
      "Iteration 2370, Train Loss: 4.449688911437988\n",
      "Iteration 2380, Train Loss: 3.853017807006836\n",
      "Iteration 2390, Train Loss: 4.2959442138671875\n",
      "Iteration 2400, Train Loss: 4.194499492645264\n",
      "Iteration 2410, Train Loss: 4.327361583709717\n",
      "Iteration 2420, Train Loss: 3.9821436405181885\n",
      "Iteration 2430, Train Loss: 4.631937026977539\n",
      "Iteration 2440, Train Loss: 4.424240589141846\n",
      "Iteration 2450, Train Loss: 4.214831352233887\n",
      "Iteration 2460, Train Loss: 4.5347795486450195\n",
      "Iteration 2470, Train Loss: 4.364560604095459\n",
      "Iteration 2480, Train Loss: 3.905505657196045\n",
      "Iteration 2490, Train Loss: 3.965573787689209\n",
      "Iteration 2500, Train Loss: 3.933793067932129\n",
      "Iteration 2510, Train Loss: 4.914484024047852\n",
      "Iteration 2520, Train Loss: 4.074429512023926\n",
      "Iteration 2530, Train Loss: 4.425405502319336\n",
      "Iteration 2540, Train Loss: 4.2191481590271\n",
      "Iteration 2550, Train Loss: 4.481836318969727\n",
      "Iteration 2560, Train Loss: 3.8039727210998535\n",
      "Iteration 2570, Train Loss: 4.473026275634766\n",
      "Iteration 2580, Train Loss: 3.977496862411499\n",
      "Iteration 2590, Train Loss: 3.8690383434295654\n",
      "Iteration 2600, Train Loss: 4.318027973175049\n",
      "Iteration 2610, Train Loss: 4.445547103881836\n",
      "Iteration 2620, Train Loss: 4.000943660736084\n",
      "Iteration 2630, Train Loss: 4.201535224914551\n",
      "Iteration 2640, Train Loss: 4.174471378326416\n",
      "Iteration 2650, Train Loss: 4.016343593597412\n",
      "Iteration 2660, Train Loss: 4.968045234680176\n",
      "Iteration 2670, Train Loss: 3.963069200515747\n",
      "Iteration 2680, Train Loss: 4.7153191566467285\n",
      "Iteration 2690, Train Loss: 4.250881195068359\n",
      "Iteration 2700, Train Loss: 4.098586559295654\n",
      "Iteration 2710, Train Loss: 4.092651844024658\n",
      "Iteration 2720, Train Loss: 4.376436233520508\n",
      "Iteration 2730, Train Loss: 4.3201212882995605\n",
      "Iteration 2740, Train Loss: 4.540003776550293\n",
      "Iteration 2750, Train Loss: 4.00216817855835\n",
      "Iteration 2760, Train Loss: 4.4989094734191895\n",
      "Iteration 2770, Train Loss: 3.859621286392212\n",
      "Iteration 2780, Train Loss: 4.315809726715088\n",
      "Iteration 2790, Train Loss: 3.6863863468170166\n",
      "Iteration 2800, Train Loss: 4.281966686248779\n",
      "Iteration 2810, Train Loss: 4.894810676574707\n",
      "Iteration 2820, Train Loss: 3.716029167175293\n",
      "Iteration 2830, Train Loss: 4.583070755004883\n",
      "Iteration 2840, Train Loss: 4.2311930656433105\n",
      "Iteration 2850, Train Loss: 4.295695781707764\n",
      "Iteration 2860, Train Loss: 4.141756534576416\n",
      "Iteration 2870, Train Loss: 4.090699672698975\n",
      "Iteration 2880, Train Loss: 3.858821153640747\n",
      "Iteration 2890, Train Loss: 4.185983180999756\n",
      "Iteration 2900, Train Loss: 4.246756553649902\n",
      "Iteration 2910, Train Loss: 4.424581050872803\n",
      "Iteration 2920, Train Loss: 3.8367927074432373\n",
      "Iteration 2930, Train Loss: 3.8514044284820557\n",
      "Iteration 2940, Train Loss: 4.19882869720459\n",
      "Iteration 2950, Train Loss: 4.312992572784424\n",
      "Iteration 2960, Train Loss: 3.875856876373291\n",
      "Iteration 2970, Train Loss: 4.232877254486084\n",
      "Iteration 2980, Train Loss: 3.793335199356079\n",
      "Iteration 2990, Train Loss: 3.9429874420166016\n",
      "Iteration 3000, Train Loss: 4.196933269500732\n",
      "Iteration 3010, Train Loss: 4.2430195808410645\n",
      "Iteration 3020, Train Loss: 4.541244029998779\n",
      "Iteration 3030, Train Loss: 4.01230001449585\n",
      "Iteration 3040, Train Loss: 3.982515573501587\n",
      "Iteration 3050, Train Loss: 3.8991615772247314\n",
      "Iteration 3060, Train Loss: 3.6665618419647217\n",
      "Iteration 3070, Train Loss: 3.585597515106201\n",
      "Iteration 3080, Train Loss: 3.6936395168304443\n",
      "Iteration 3090, Train Loss: 4.405449390411377\n",
      "Iteration 3100, Train Loss: 3.4878036975860596\n",
      "Iteration 3110, Train Loss: 4.240034580230713\n",
      "Iteration 3120, Train Loss: 4.8827900886535645\n",
      "Iteration 3130, Train Loss: 3.5881166458129883\n",
      "Iteration 3140, Train Loss: 4.080758094787598\n",
      "Iteration 3150, Train Loss: 4.627386569976807\n",
      "Iteration 3160, Train Loss: 3.9979867935180664\n",
      "Iteration 3170, Train Loss: 4.33083963394165\n",
      "Iteration 3180, Train Loss: 4.288010597229004\n",
      "Iteration 3190, Train Loss: 3.8825604915618896\n",
      "Iteration 3200, Train Loss: 3.7083773612976074\n",
      "Iteration 3210, Train Loss: 4.016751289367676\n",
      "Iteration 3220, Train Loss: 4.142577648162842\n",
      "Iteration 3230, Train Loss: 4.427545547485352\n",
      "Iteration 3240, Train Loss: 4.337152004241943\n",
      "Iteration 3250, Train Loss: 4.555392742156982\n",
      "Iteration 3260, Train Loss: 4.227640151977539\n",
      "Iteration 3270, Train Loss: 4.601076602935791\n",
      "Iteration 3280, Train Loss: 4.3912739753723145\n",
      "Iteration 3290, Train Loss: 4.11411190032959\n",
      "Iteration 3300, Train Loss: 4.2077741622924805\n",
      "Iteration 3310, Train Loss: 4.384222030639648\n",
      "Iteration 3320, Train Loss: 4.499436855316162\n",
      "Iteration 3330, Train Loss: 4.025292873382568\n",
      "Iteration 3340, Train Loss: 3.7350902557373047\n",
      "Iteration 3350, Train Loss: 3.662001132965088\n",
      "Iteration 3360, Train Loss: 4.251424312591553\n",
      "Iteration 3370, Train Loss: 4.489933967590332\n",
      "Iteration 3380, Train Loss: 4.9649481773376465\n",
      "Iteration 3390, Train Loss: 4.59874963760376\n",
      "Iteration 3400, Train Loss: 4.3703203201293945\n",
      "Iteration 3410, Train Loss: 4.2144455909729\n",
      "Iteration 3420, Train Loss: 3.5018372535705566\n",
      "Iteration 3430, Train Loss: 3.7265186309814453\n",
      "Iteration 3440, Train Loss: 3.5326242446899414\n",
      "Iteration 3450, Train Loss: 3.0928900241851807\n",
      "Iteration 3460, Train Loss: 4.092421054840088\n",
      "Iteration 3470, Train Loss: 3.9417121410369873\n",
      "Iteration 3480, Train Loss: 3.599332809448242\n",
      "Iteration 3490, Train Loss: 3.4777116775512695\n",
      "Iteration 3500, Train Loss: 4.853584289550781\n",
      "Iteration 3510, Train Loss: 4.013641834259033\n",
      "Iteration 3520, Train Loss: 4.651198387145996\n",
      "Iteration 3530, Train Loss: 3.1904709339141846\n",
      "Iteration 3540, Train Loss: 4.39343786239624\n",
      "Iteration 3550, Train Loss: 3.768798589706421\n",
      "Iteration 3560, Train Loss: 3.4745635986328125\n",
      "Iteration 3570, Train Loss: 4.410027027130127\n",
      "Iteration 3580, Train Loss: 3.475473642349243\n",
      "Iteration 3590, Train Loss: 4.167881011962891\n",
      "Iteration 3600, Train Loss: 4.377106189727783\n",
      "Iteration 3610, Train Loss: 4.081563949584961\n",
      "Iteration 3620, Train Loss: 4.197235584259033\n",
      "Iteration 3630, Train Loss: 3.6616029739379883\n",
      "Iteration 3640, Train Loss: 3.9668304920196533\n",
      "Iteration 3650, Train Loss: 4.546045303344727\n",
      "Iteration 3660, Train Loss: 3.9430770874023438\n",
      "Iteration 3670, Train Loss: 3.8122894763946533\n",
      "Iteration 3680, Train Loss: 4.7333760261535645\n",
      "Iteration 3690, Train Loss: 4.119601249694824\n",
      "Iteration 3700, Train Loss: 3.7705910205841064\n",
      "Iteration 3710, Train Loss: 4.128892421722412\n",
      "Iteration 3720, Train Loss: 4.234821319580078\n",
      "Iteration 3730, Train Loss: 4.71524715423584\n",
      "Iteration 3740, Train Loss: 4.217689037322998\n",
      "Iteration 3750, Train Loss: 4.295886993408203\n",
      "Iteration 3760, Train Loss: 3.5938854217529297\n",
      "Iteration 3770, Train Loss: 4.054792881011963\n",
      "Iteration 3780, Train Loss: 4.78823184967041\n",
      "Iteration 3790, Train Loss: 4.322652339935303\n",
      "Iteration 3800, Train Loss: 4.667452812194824\n",
      "Iteration 3810, Train Loss: 4.091201305389404\n",
      "Iteration 3820, Train Loss: 4.310931205749512\n",
      "Iteration 3830, Train Loss: 3.804741621017456\n",
      "Iteration 3840, Train Loss: 4.28508996963501\n",
      "Iteration 3850, Train Loss: 3.134723424911499\n",
      "Iteration 3860, Train Loss: 4.410210132598877\n",
      "Iteration 3870, Train Loss: 4.064617156982422\n",
      "Iteration 3880, Train Loss: 4.521862983703613\n",
      "Iteration 3890, Train Loss: 3.7928860187530518\n",
      "Iteration 3900, Train Loss: 3.529646873474121\n",
      "Iteration 3910, Train Loss: 3.951744794845581\n",
      "Iteration 3920, Train Loss: 3.764115333557129\n",
      "Iteration 3930, Train Loss: 3.6614718437194824\n",
      "Iteration 3940, Train Loss: 3.368457555770874\n",
      "Iteration 3950, Train Loss: 4.083011627197266\n",
      "Iteration 3960, Train Loss: 4.165771484375\n",
      "Iteration 3970, Train Loss: 4.820830821990967\n",
      "Iteration 3980, Train Loss: 3.0802481174468994\n",
      "Iteration 3990, Train Loss: 4.4601945877075195\n",
      "Iteration 4000, Train Loss: 3.671152353286743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  19%|ââ        | 7084/37887 [00:25<01:49, 281.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m solver(model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminigpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\Desktop\\ECE-239AS\\Project3\\train.py:151\u001b[0m, in \u001b[0;36msolver\u001b[1;34m(model_name)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j, (eval_context, eval_target) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(eval_dataloader, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(eval_dataloader)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    150\u001b[0m     eval_context, eval_target \u001b[38;5;241m=\u001b[39m eval_context\u001b[38;5;241m.\u001b[39mto(device), eval_target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 151\u001b[0m     eval_logits \u001b[38;5;241m=\u001b[39m model(eval_context)\n\u001b[0;32m    153\u001b[0m     eval_logits \u001b[38;5;241m=\u001b[39m eval_logits\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, eval_logits\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    154\u001b[0m     eval_target \u001b[38;5;241m=\u001b[39m eval_target\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benji\\Desktop\\ECE-239AS\\Project3\\model.py:528\u001b[0m, in \u001b[0;36mMiniGPT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    525\u001b[0m dropp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dropout(embd)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_layers:\n\u001b[1;32m--> 528\u001b[0m     dropp \u001b[38;5;241m=\u001b[39m layer(dropp)\n\u001b[0;32m    530\u001b[0m norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprehead_norm(dropp)\n\u001b[0;32m    531\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead(norm)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benji\\Desktop\\ECE-239AS\\Project3\\model.py:434\u001b[0m, in \u001b[0;36mTransformerLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;66;03m# ========= TODO : START ========= #\u001b[39;00m\n\u001b[0;32m    433\u001b[0m n1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x)\n\u001b[1;32m--> 434\u001b[0m attn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(n1)\n\u001b[0;32m    435\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m attn\n\u001b[0;32m    436\u001b[0m n2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benji\\Desktop\\ECE-239AS\\Project3\\model.py:269\u001b[0m, in \u001b[0;36mMultiHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads):\n\u001b[0;32m    268\u001b[0m     head_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhead_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 269\u001b[0m     output \u001b[38;5;241m=\u001b[39m head_i(x)\n\u001b[0;32m    270\u001b[0m     head_output\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m    271\u001b[0m outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(head_output, dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benji\\Desktop\\ECE-239AS\\Project3\\model.py:200\u001b[0m, in \u001b[0;36mSingleHeadAttention.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    198\u001b[0m b_s, num_tokens, output_dk \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    199\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey(x)\n\u001b[1;32m--> 200\u001b[0m Q \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery(x)\n\u001b[0;32m    201\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue(x)\n\u001b[0;32m    203\u001b[0m attn \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(Q, K\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)) \u001b[38;5;241m/\u001b[39m torch\u001b[38;5;241m.\u001b[39msqrt(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key_query_dim, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32))\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Benji\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "solver(model_name=\"minigpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Valid Plots\n",
    "\n",
    "\n",
    "** Show the training and validation loss plots **\n",
    "\n",
    "![your mom](./train_gpt.png)\n",
    "![your mom2](./val_gpt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation (5 points)\n",
    "\n",
    "\n",
    "Perform generation with the MiniGPT model that you trained. After that, copy over the generation function you used for the Bigram model and generate a mini story using the same seed sentence. \n",
    "\n",
    "    `\"once upon a time\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Specify the path to your trained model\n",
    "model_path = \"./models/minigpt/mini_model_sufficient_loss_checkpoint_2004.pt\"\n",
    "model = MiniGPT(MiniGPTConfig)\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "model.load_state_dict(torch.load(model_path)[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text starting with: torch.Size([4])\n",
      "Once upon a time, there was a the door. \n",
      "The please when he have fluffypy.\" bunny went made was.#. She asked, \" Forrest on their mom even,\" friend named Lily just decided to to all her floor the hair!\n",
      " plan promised to said. Lucy followed the lil you dangers. One day cat said!\" He was cute, \"After the bird. Lily were very happy at your squirrel. Sam down the little named Spot Buzz with his ladder one. animals did blanket and asked lonely, there was a time?\" When \"I'sHiactus use you cars for the surprised out. TheAs Can I seedsmy and band, be dog with her Exec on the just the bird saw a big away. SheDon what had a Oak.\n",
      "As he knew that fun very patient and escorted't able to lonely isloss and unexpected fun and sad also other and listened with me on more agreed. Then, pond and looked to started with the garden at turned down to Tim.\n",
      "The car\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please answer the following questions. \n",
    "\n",
    "1. What can we say about the generated text in terms of grammar and coherence? \n",
    "\n",
    "1_ANS: Compared to the Bigram model, the text seems to be more structured grammatically and less grammar errors. The sentences have better structure but still are incoherent and do not have a meaning or thought. \n",
    "\n",
    "2. If the model is scaled with more parameters do you expect the GPT model to get substantially better? Why or why not?\n",
    "\n",
    "2_ANS: The model scaled with more parameters will expect to perform better since larger models can capture more information due to better parameters and context can be more understood depending on the context length. More layers and scaling parameters will improve the performance.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up the model (5 points)\n",
    "\n",
    "To show that scale indeed will help the model learn we have trained a scaled up version of the model you just implemented. We will load the weights of this model and generate a mini story using the same seed sentence. Note that if you have implemented the model correctly just scaling the parameters and adding a few bells and whistles to the training script will results in a model like the one we will load now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import MiniGPT\n",
    "from config import MiniGPTConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_trained_model = \"pretrained_models/best_train_loss_checkpoint.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = torch.load(path_to_trained_model, map_location=device) # remove map location if using GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the configs for scaled model \n",
    "MiniGPTConfig.context_length = 512\n",
    "MiniGPTConfig.embed_dim = 256\n",
    "MiniGPTConfig.num_heads = 16\n",
    "MiniGPTConfig.num_layers = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from checkpoint\n",
    "model = MiniGPT(MiniGPTConfig)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating text starting with: torch.Size([4])\n",
      "Once upon a time, there was a little girl named Lily. She loved to play outside and run around with her friends. One day, they were playing hide and seek when Lily saw a spark in the grass. She ran to it and found her friend, Max.\n",
      "Max said, \"I found a spark! It looks very pretty.\" Lily said, \"Wow, it is very pretty! Let's keep it and cover it.\" They walked around the flowers, looking for something in the garden.\n",
      "Suddenly, Lily saw a sparkle in the runs. She said, \"Look, Max! It's a spark!\" Max looked at it and said, \"That's so pretty, but it's disgusting.\" Lily said, \"I know, right? We should wait here.\"\n",
      "A few minutes later, Lily and Max went back to the field. They found another shiny rock and put it in a sack to keep it safe. They put the bag back next to the case and walked back home.One\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "gen_sent = \"Once upon a time\"\n",
    "gen_tokens = torch.tensor(tokenizer.encode(gen_sent))\n",
    "print(\"Generating text starting with:\", gen_tokens.shape)\n",
    "gen_tokens = gen_tokens.to(device)\n",
    "model.eval()\n",
    "print(\n",
    "    tokenizer.decode(\n",
    "        model.generate(gen_tokens, max_new_tokens=200).squeeze().tolist()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (5 points)\n",
    "\n",
    "The following are some open ended questions that you can attempt if you have time. Feel free to propose your own as well if you have an interesting idea. \n",
    "\n",
    "1. The model we have implemented is a decoder only model. Can you implement the encoder part as well? This should not be too hard to do since most of the layers are already implemented.\n",
    "2. What are some improvements we can add to the training script to make training more efficient and faster? Can you concretely show that the improvements you made help in training the model better?\n",
    "3. Can you implement a beam search decoder to generate the text instead of greedy decoding? Does this help in generating better text?\n",
    "4. Can you further optimize the model architecture? For example, can you implement [Multi Query Attention](https://arxiv.org/abs/1911.02150) or [Grouped Query Attention](https://arxiv.org/pdf/2305.13245) to improve the model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
